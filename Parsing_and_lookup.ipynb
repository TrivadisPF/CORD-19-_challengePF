{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unidecode tqdm pandas rdflib SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path_header = r'CORD-19-research-challenge/2020-03-13/'):\n",
    "    import pandas as pd\n",
    "    path_to_csv = path_header + r\"all_sources_metadata_2020-03-13.csv\"\n",
    "    csv_df = pd.read_csv(path_to_csv)\n",
    "    return csv_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsons(path_header = r'CORD-19-research-challenge/2020-03-13/'):\n",
    "    import os, json\n",
    "    import pandas as pd\n",
    "\n",
    "    # this finds our json files\n",
    "    path_to_json = path_header + r'biorxiv_medrxiv/biorxiv_medrxiv'\n",
    "    path_to_json_2 = path_header + r\"comm_use_subset/comm_use_subset\"\n",
    "    path_to_json_3 = path_header + r\"noncomm_use_subset/noncomm_use_subset\"\n",
    "    path_to_json_4 = path_header + r\"pmc_custom_license/pmc_custom_license\"\n",
    "\n",
    "\n",
    "    list_of_jsons= [path_to_json, path_to_json_2, path_to_json_3,path_to_json_4]\n",
    "\n",
    "    json_files = []\n",
    "\n",
    "    for i in list_of_jsons:\n",
    "        json_files.extend([os.path.join(i,pos_json) for pos_json in os.listdir(i) if pos_json.endswith('.json')])\n",
    "\n",
    "\n",
    "\n",
    "    #json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "\n",
    "    # here I define my pandas Dataframe with the columns I want to get from the json\n",
    "    jsons_data = pd.DataFrame(columns=['id', \"title\", \"paper_abstract\",\"paper_body\"])\n",
    "\n",
    "    # we need both the json and an index number so use enumerate()\n",
    "\n",
    "    for index, js in enumerate(json_files):\n",
    "        with open(js) as json_file:\n",
    "            json_text = json.load(json_file)\n",
    "            #print(json_text)\n",
    "            # here you need to know the layout of your json and each json has to have\n",
    "            # the same structure (obviously not the structure I have here)\n",
    "            title = json_text['metadata']['title']\n",
    "\n",
    "            paper_id = json_text[\"paper_id\"]\n",
    "\n",
    "            #reduces the list only if there is content\n",
    "            if json_text[\"abstract\"] != []:\n",
    "                paper_abstract = (json_text[\"abstract\"])\n",
    "            else:\n",
    "                paper_abstract = (json_text[\"abstract\"])\n",
    "\n",
    "            #paper_abstract = (json_text[\"abstract\"])\n",
    "            paper_body = (json_text[\"body_text\"])\n",
    "            #print(title)\n",
    "            # here I push a list of data into a pandas DataFrame at row given by 'index'\n",
    "            jsons_data.loc[index] = [paper_id, title, paper_abstract, paper_body]\n",
    "\n",
    "    return jsons_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = read_jsons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the frames together\n",
    "combined_data = csv_df.merge(jsons, left_on='sha',right_on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv(\"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used this to buffer the extraction\n",
    "import pandas as pd\n",
    "combined_data = pd.read_csv(\"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a fulltest column\n",
    "# for this use the title_x (if Not NaN), title_x, all fields from the abstract, and all fields from the paper body together\n",
    "\n",
    "def parse_json_section(text):\n",
    "    import json\n",
    "    if isinstance(text, float) :\n",
    "        return \"\"\n",
    "    else :\n",
    "        sep = r\" \"\n",
    "        whole_text_as_string = \"\"\n",
    "        for section in text:\n",
    "            #print(section['text'])\n",
    "            whole_text_as_string += section['text'] + sep\n",
    "        return whole_text_as_string\n",
    "    \n",
    "\n",
    "def make_catchall_field(row):\n",
    "    sep = r\" \"\n",
    "    title_x = row['title_x']\n",
    "    title_y = row['title_y']\n",
    "    abstract = row['abstract']\n",
    "    abstract_fields = row['paper_abstract']\n",
    "    body_fields = row['paper_body']\n",
    "    abstract_text = parse_json_section(abstract_fields)\n",
    "    body_text = parse_json_section(body_fields)\n",
    "    \n",
    "    catch_all = str(title_x) + sep + str(title_y) + sep + str(abstract) + sep + str(abstract_text) + sep + str(body_text)\n",
    "    return catch_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "combined_data['catchall'] = combined_data.progress_apply(lambda row: make_catchall_field(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv(\"combined_data_with_catchall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used this to buffer the extraction\n",
    "import pandas as pd\n",
    "combined_data = pd.read_csv(\"combined_data_with_catchall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we need to annotate that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return str(text).encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "def analyze_text(text):\n",
    "    import requests \n",
    "    # do some cleansing of this text    \n",
    "    # defining the api-endpoint \n",
    "    API_ENDPOINT = 'http://nlu:8080/factextraction/analyze'\n",
    "    headers = {\"accept\": \"application/json\", \"content-type\": \"application/json\"}\n",
    "    # data to be sent to api\n",
    "    #print(\"Text sent to ambiverse is: \", remove_non_ascii(text))\n",
    "    data = r'{\"docId\": \"doc2\", \"text\":\"' + remove_non_ascii(text) + r'\", \"extractConcepts\": \"true\", \"language\": \"en\" }'\n",
    "    # sending post request and saving response as response object \n",
    "    r = requests.post(url = API_ENDPOINT, data = data, headers = headers) \n",
    "    #print(\"returned text is: \", r.text)\n",
    "    return r.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging only \n",
    "#from tqdm import tqdm\n",
    "#import swifter\n",
    "#jsons['out'] = jsons['title'].swifter.progress_bar(True).set_npartitions(npartitions=4).apply(analyze_text, axis=1)\n",
    "#jsons['title_annotated'] = jsons.swifter.allow_dask_on_strings().progress_bar(True).set_npartitions(npartitions=8).apply(lambda row: analyze_text(row['title']), axis = 1)\n",
    "\n",
    "#tqdm.pandas()\n",
    "#jsons['title_annotated'] = jsons[0:12].apply(lambda row: analyze_text(row['title']), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from here: https://www.kaggle.com/mlwhiz/parallelization-kernel\n",
    "from multiprocessing import  Pool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parallelized_apply(df, func, numProcs=4):\n",
    "    df_split = np.array_split(df, numProcs)\n",
    "    pool = Pool(numProcs) \n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def runAnnotate(df):\n",
    "    # add here also the functions for all other columns\n",
    "    df['catchall_annotated'] =  df.progress_apply(lambda row: analyze_text(row['catchall']), axis = 1)\n",
    "    return df\n",
    "\n",
    "# In theory every document annotation runs in a seperate thread, they all go to a single db (this db could handle a lot of parallel annotation threads)\n",
    "# Every annotation thread should have 8 GB of memory, so running 10 annotation threads in parallel would need 80 GB of memory. \n",
    "# Be careful with the memory monitor (e.g. htop), the db just shows the in memory indices as cached pages (yellor/orange bars in htop), \n",
    "# if you consume more memory elsewhere, then these pages are stored back to disk and you loose performance.\n",
    "\n",
    "# data_annotated = parallelized_apply(combined_data[0:399], runAnnotate, numProcs = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it first for the abstracts only\n",
    "abstracts_only = combined_data[combined_data.has_full_text != True]\n",
    "abstracts_annotated = parallelized_apply(abstracts_only, runAnnotate, numProcs = 12) \n",
    "abstracts_annotated.to_csv('abstracts_annotated.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_annotated.to_csv('abstracts_annotated.csv', index=True)\n",
    "abstracts_annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then for the fulltexts\n",
    "full_texts_only = combined_data[combined_data.has_full_text == True]\n",
    "full_texts_annotated = parallelized_apply(full_texts_only, runAnnotate, numProcs = 8)\n",
    "full_texts_annotated.to_csv('full_texts_annotated.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts_annotated.to_csv('full_texts_annotated.csv', index=True)\n",
    "full_texts_annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_annotated = pd.concat(abstracts_annotated, full_texts_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to disk temporarly\n",
    "data_annotated.to_csv('data_annotated.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back the data\n",
    "import pandas as pd\n",
    "abstracts_annotated = pd.read_csv(\"abstracts_annotated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will want to get all further wikidata entity information to it, so we will need to load them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 16281/16281 [09:39<00:00, 28.10it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed:  11313694  triples into the graph\n",
      "Parsing failed on  ['3c4e7b941e30be5bcd8c5222305bb53b5374f70c', 'c74cbed3524e1dd5d93d9db9d71c9f91ba8561e0', '9c86de3d1107dd2c0fb8d9bb03fff120c41d6559', '283fd3dad298787130f03473882a3dbf22420dc4', 'ce545fcd547a3bf435499dd84b5c87332a5c5879', '5baa1fa52d730151424cc2a70242c7937016b815', 'c163182040644bbb1350c3d3acacacc431fb07aa', '93f5284236b5257175bfff402f8b8ea90e0a79c0', 'dfddc7262329261ef41c6da4bd8a56ec2c3f81dc', 'c9e36f5f3564f04bfbbcefe1d1eec41078b2eadd', '80117863dd2a5d88ad1c777af6952bd136811a25', '7e0a164300efd5ab4d734ae591fcd8b4ef3554f8', '8ac43920461faa8a180d94acd6c5a6b35ab8bf7a', 'f13019c1409c978d7d11cdbbbb4b2c16abfc1bd7', 'f83277cbba652f6ef2a9893a3d470cdf6580979d', 'ae6773868c2850e5c8b8177735b19d01341a53b6', '6389f677ca08ac014611fe48723556b78dbd8744', '6318351120824de3f612d0dbdd9611f4c94bf071', 'd9d02e04db1c0d620a206b018960427447bfeff6', 'd08c4a6506c1a9b8fe7ace82d4c14f9636e1c33a', '0f7bb2b30b0eba1a065a6dfc88dbbd99053ff1ba', 'bdfca78549a3a21e5774810537a3eaa0390b8e59', '5b6a62811eb3feb32f20a0193f43c435292fb6ec', '524738e8e638c805f7c00ccbafb2821e8cbae115', '1bcbb56b7ec449ef48d930dce2f02fdc10aeb3e7', '562482615b134bd07325e607832d9d67aaa13e60', '7d6e1512b86e2006936b67f5a39f117330e2c98e', 'f5f49c96623a02ef068134f9f468602f5e4763f4', '251b7d1d3aa75088629d05bfab8ea46b6356b791', 'e95912dc0ea265d0559e79f33c0c9c9fb1834b6a', '537abde4c5b849f55e62ff102f4f91dfca742ff7', '835c46cb0103ede80257c6b4d5c710185f88ebf4', 'e79419067a6b140a71e7a89e5a275ce2bf89ac35', 'c37f596934a7944a47229af9491eaeae4f595528', '049cf5f0e55a0f6b472630b01aff93cffffb0ece', '0efea89bfa335dbd71828c4ed690012ca36fe1ce', '829a94ca9817bb3e0ce0373ce733fe751cb922ec', 'e2a54d852edc4fc5b2e136f5840a5b07631bdae9', 'ac3dcfecd7f5486d24470895e9248688b812af9e', 'ac49bb9b1f83ff26a1ffda195c8f06b9cdf72b3a', '017015a89a8532f7b4a05bc06126d9e4413d2342', '88356a43079eda96b1416bb12b8eca07a73d4f02', 'b345fe36029c6ccebee177f8635ea0a5ab5f0141', 'fd193c4454e959a01d62a4a31358b9aca9422bba', '2f787a3576169eb58da0ec737994385190b024d6', '8666c0d92dc4d0de2839600559228d4dd07f487f', '590425a9891786425ac88780f5fd37414cfb1466', '4eea77457606a86e9c391837029ab326868b9fa7', 'd67640da9a65493bdc6c39501c483b723c722fcb', 'd7e49619dea5bcfca63185a869586c0685d4a266', 'cac461b8650d4c26c02f44c3ec815f8de41595aa', '8c15e0d2ead3c4f2f977bff7c4c54aeefbb90872', '030bcc13b2088bcfc2b1c2f6117e349e13f2e57b', 'ebaa3d997fcc4b0ba9d69664e4c11d76b7ab9ff1', '5486c2563d83a0e3ad74b1d703596bdceacc76b3', '233b60dfe02f8c61b4a962657475c5bf8c72dfbd', '5df0eff67085765ddb57c2d6c684af9caef895e7', 'd1419f840d41fa492861571bcd2b0d3d7f8dfaaa', 'ca1a65f1df55811d14bb73bf71fb900b4f36a46a', '3b3150852230121eab4302b9d423816d298b5419', 'a73fc4cdae879883ba1f8d2fb4af2b72823dca8d', '2a274254f86c68ef4ae452143fc35e9cae52d77d', 'a3a52497caf1eea460b96a1e5dc9fb9898878045', 'd8086566c63a32aa57e8fe541d3877879568a1fa', '33709ad4554cfe57440d6186720cbbc56821f18a', '0e1b33290ff06012a46d64a044a4a69f60fb7513', 'c55c92481de42668c67f0ef7da1431c70874075e', 'f19467ad896da62d9f35816c4d54d75c0237d99d', '4f3d6010cea5322e31ea6785a8fa1f9265e4d44a']\n"
     ]
    }
   ],
   "source": [
    "# deassable the jsons first\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph\n",
    "from rdflib.namespace import RDF, FOAF, RDFS\n",
    "import urllib.parse\n",
    "\n",
    "# bob = URIRef(\"http://example.org/people/Bob\")\n",
    "# linda = BNode() # a GUID is generated\n",
    "# name = Literal('Bob') # passing a string\n",
    "# age = Literal(24) # passing a python int\n",
    "# height = Literal(76.5) # passing a python float\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph_root = \"http://www.trivadis.com/kg/\"\n",
    "DCTYPE = Namespace(\"http://purl.org/dc/dcmitype/\")\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "# links from a document\n",
    "hasMention = URIRef(\"http://www.trivadis.com/kg/ontology/hasMention\")\n",
    "hasFact = URIRef(\"http://www.trivadis.com/kg/ontology/hasFact\")\n",
    "\n",
    "# links from a mention to the outside\n",
    "hasConcept = URIRef(\"http://www.trivadis.com/kg/ontology/hasConcept\")\n",
    "\n",
    "# types\n",
    "mentionType = URIRef(\"http://www.trivadis.com/kg/ontology/Mention\")\n",
    "docMentionType = URIRef(\"http://www.trivadis.com/kg/ontology/DocMention\")\n",
    "factType = URIRef(\"http://www.trivadis.com/kg/ontology/Fact\")\n",
    "conceptType = URIRef(\"http://www.trivadis.com/kg/ontology/Concept\")\n",
    "\n",
    "#mention properties\n",
    "hasCharLength = URIRef(\"http://www.trivadis.com/kg/ontology/hasCharLength\")\n",
    "hasCharOffset = URIRef(\"http://www.trivadis.com/kg/ontology/hasCharOffset\")\n",
    "hasText = URIRef(\"http://www.trivadis.com/kg/ontology/hasText\")\n",
    "hasConfidence = URIRef(\"http://www.trivadis.com/kg/ontology/hasConfidence\")\n",
    "hasName = URIRef(\"http://www.trivadis.com/kg/ontology/hasName\")\n",
    "hasURL = URIRef(\"http://www.trivadis.com/kg/ontology/hasURL\")\n",
    "hasType = URIRef(\"http://www.trivadis.com/kg/ontology/hasType\")\n",
    "\n",
    "# fact specifics\n",
    "hasSubject = URIRef(\"http://www.trivadis.com/kg/ontology/hasSubject\")\n",
    "hasRelation = URIRef(\"http://www.trivadis.com/kg/ontology/hasRelation\")\n",
    "hasObject = URIRef(\"http://www.trivadis.com/kg/ontology/hasObject\")\n",
    "\n",
    "# print g.serialize(format='turtle')\n",
    "\n",
    "# keys for the json\n",
    "match_key = 'matches'\n",
    "entity_key = 'entities'\n",
    "fact_key = 'facts'\n",
    "\n",
    "error_parse = []\n",
    "\n",
    "def parse_row_to_graph(graph, row):\n",
    "    annotation = row[['catchall_annotated']][0]\n",
    "    doc_id = str(row[['sha']][0])\n",
    "    doi = row[['doi']][0]\n",
    "    catchall = row[['catchall']][0]\n",
    "\n",
    "    #print(doc_id)\n",
    "    #print(catchall)\n",
    "    #print(doi)\n",
    "        \n",
    "    theDoc = URIRef(graph_root + urllib.parse.quote(doc_id))\n",
    "    g.add( (theDoc, RDF.type, DCTYPE.Text) )\n",
    "    g.add( (theDoc, DCTERMS.identifier, Literal(doi)) )\n",
    "    g.add( (theDoc, DCTERMS.description, Literal(catchall)) )\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        json_annotation = json.loads(annotation)\n",
    "\n",
    "        #print(json_annotation)\n",
    "\n",
    "        if match_key in json_annotation:\n",
    "            # add the annotations as mentions\n",
    "            # print(\"Found matches: \")\n",
    "            for match in json_annotation[match_key]:\n",
    "                # A match is: {'charLength': 7, 'charOffset': 0, 'text': 'Imaging', 'entity': {'id': 'http://www.wikidata.org/entity/Q931309', 'confidence': 0.12630685029203295}}\n",
    "                charLength = match['charLength']\n",
    "                charOffset = match['charOffset']\n",
    "                match_text = match['text']\n",
    "\n",
    "                match_iri = URIRef(graph_root + urllib.parse.quote(doc_id + \"/mention/\" + str(charLength) + str(charOffset) + str(match_text)))\n",
    "\n",
    "                g.add((theDoc, hasMention, match_iri))\n",
    "                g.add((match_iri, RDF.type, mentionType))\n",
    "                g.add((match_iri, hasCharLength, Literal(int(charLength))))\n",
    "                g.add((match_iri, hasCharOffset, Literal(int(charOffset))))\n",
    "                g.add((match_iri, hasText, Literal(match_text)))\n",
    "                if ('entity' in match):\n",
    "                    if ('id' in match['entity'] and 'confidence' in match['entity']):\n",
    "                        match_entity = match['entity']['id']\n",
    "                        match_confidence = match['entity']['confidence']\n",
    "                        g.add((match_iri, hasConcept, URIRef(match_entity)))\n",
    "                        g.add((URIRef(match_entity), RDF.type, conceptType))\n",
    "                        g.add((match_iri, hasConfidence, Literal(float(match_confidence))))\n",
    "                # print(match)\n",
    "\n",
    "        if entity_key in json_annotation:\n",
    "            # add the annotations as mentions\n",
    "            # print(\"Found entities: \")\n",
    "            for entity in json_annotation[entity_key]:\n",
    "                # A entity is: {'id': 'http://www.wikidata.org/entity/Q931309', 'name': 'Medical imaging', 'url': 'http://en.wikipedia.org/wiki/Medical%20imaging', 'type': 'CONCEPT', 'salience': 0.0}\n",
    "                match_entity = entity['id']\n",
    "                match_name = entity['name']\n",
    "                match_url = entity['url']\n",
    "                match_type = entity['type']\n",
    "                match_confidence = entity['salience']\n",
    "                match_iri = URIRef(graph_root + urllib.parse.quote(doc_id + \"/entity/\" + str(match_name) + str(match_type)))\n",
    "\n",
    "                g.add((theDoc, hasMention, match_iri))\n",
    "                g.add((match_iri, RDF.type, docMentionType))\n",
    "                g.add((match_iri, hasConfidence, Literal(float(match_confidence))))\n",
    "                g.add((match_iri, hasName, Literal(match_name)))\n",
    "                g.add((match_iri, hasConcept, URIRef(match_entity)))\n",
    "                g.add((URIRef(match_entity), RDF.type, conceptType ))\n",
    "                g.add((match_iri, hasURL, Literal(match_url)))\n",
    "                g.add((match_iri, hasType, Literal(match_type)))\n",
    "                # print(entity)\n",
    "\n",
    "        if fact_key in json_annotation:\n",
    "            # add the annotations as facts\n",
    "            # print(\"Found facts: \")\n",
    "            for fact in json_annotation[fact_key]:\n",
    "                # A fact is: {'subject': {'text': 'RETRACTED Chinese medical staff', 'charOffset': 0, 'charLength': 32}, 'relation': {'text': 'request', 'charOffset': 33, 'charLength': 7}, 'object': {'text': 'international medical assistance in fighting against COVID-19 nan nan', 'charOffset': 41, 'charLength': 69}}\n",
    "                # in the fact we do not have any entities annotated, we need to do that later with the char offsets the fact has a subject, relation and object each of them are mentions\n",
    "\n",
    "                fact_subject_text = fact['subject']['text']\n",
    "                fact_subject_charLength = fact['subject']['charLength']\n",
    "                fact_subject_charOffset = fact['subject']['charOffset']\n",
    "\n",
    "                fact_relation_text = fact['relation']['text']\n",
    "                fact_relation_charLength = fact['relation']['charLength']\n",
    "                fact_relation_charOffset = fact['relation']['charOffset']\n",
    "\n",
    "                fact_iri = URIRef(graph_root + urllib.parse.quote(doc_id + \"/fact/\" + fact_relation_text +  str(fact_relation_charLength) +  str(fact_relation_charOffset)))\n",
    "                fact_subject_iri = URIRef(graph_root + urllib.parse.quote(doc_id + \"/fact/mention/\" + fact_subject_text +  str(fact_subject_charLength) +  str(fact_subject_charOffset)))\n",
    "                fact_relation_iri = URIRef(graph_root + urllib.parse.quote(doc_id + \"/fact/mention/\" + fact_relation_text +  str(fact_relation_charLength) +  str(fact_relation_charOffset)))\n",
    "\n",
    "\n",
    "                g.add((theDoc, hasFact, fact_iri))\n",
    "                g.add((fact_iri, RDF.type, factType))\n",
    "                g.add((fact_iri, hasSubject, fact_subject_iri))\n",
    "                g.add((fact_iri, hasRelation, fact_relation_iri))\n",
    "\n",
    "\n",
    "                g.add((fact_subject_iri, RDF.type, mentionType))\n",
    "                g.add((fact_subject_iri, RDF.type, factType))\n",
    "                g.add((fact_subject_iri, hasCharLength, Literal(int(fact_subject_charLength))))\n",
    "                g.add((fact_subject_iri, hasCharOffset, Literal(int(fact_subject_charOffset))))\n",
    "                g.add((fact_subject_iri, hasText, Literal(fact_subject_text)))\n",
    "\n",
    "                g.add((fact_relation_iri, RDF.type, mentionType))\n",
    "                g.add((fact_relation_iri, RDF.type, factType))\n",
    "                g.add((fact_relation_iri, hasCharLength, Literal(int(fact_relation_charLength))))\n",
    "                g.add((fact_relation_iri, hasCharOffset, Literal(int(fact_relation_charOffset))))\n",
    "                g.add((fact_relation_iri, hasText, Literal(fact_relation_text)))\n",
    "\n",
    "                # adding the object if it exists\n",
    "                if ('object' in fact):\n",
    "                    if ('text' in fact['object'] and 'charLength' in fact['object'] and 'charOffset' in fact['object']):\n",
    "                        fact_object_text = fact['object']['text']\n",
    "                        fact_object_charLength = fact['object']['charLength']\n",
    "                        fact_object_charOffset = fact['object']['charOffset']\n",
    "                        fact_object_iri = URIRef(graph_root + urllib.parse.quote(doc_id + \"/fact/mention/\" + fact_object_text +  str(fact_object_charLength) +  str(fact_object_charOffset)))\n",
    "                        g.add((fact_iri, hasObject, fact_object_iri))\n",
    "                        g.add((fact_object_iri, RDF.type, mentionType))\n",
    "                        g.add((fact_object_iri, RDF.type, factType))\n",
    "                        g.add((fact_object_iri, hasCharLength, Literal(int(fact_object_charLength))))\n",
    "                        g.add((fact_object_iri, hasCharOffset, Literal(int(fact_object_charOffset))))\n",
    "                        g.add((fact_object_iri, hasText, Literal(fact_object_text)))\n",
    "\n",
    "                # print(fact)\n",
    "\n",
    "    except:\n",
    "        if doc_id != \"nan\":\n",
    "            error_parse.append(doc_id)\n",
    "\n",
    "            \n",
    "g = Graph()\n",
    "# just using two documents\n",
    "# abstracts_annotated[0:2].progress_apply(lambda row: parse_row_to_graph(g, row), axis = 1)\n",
    "\n",
    "# running on all:\n",
    "abstracts_annotated.progress_apply(lambda row: parse_row_to_graph(g, row), axis = 1)\n",
    "\n",
    "print(\"Parsed: \", len(g), \" triples into the graph\")\n",
    "print(\"Parsing failed on \", error_parse)\n",
    "g.serialize(destination='abstracts_graph.nt', format='nt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add some analytics after this cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the instance of (P31) and the subclass of (P279) relations along each path from wikidata, we need all intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And ask the questions on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(jsons_annotated.iloc[13000:13001].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me the articles talking about \"behaviour\" : https://www.wikidata.org/wiki/Q9332 in their headline:\n",
    "behaviour_in_title = jsons_annotated[jsons_annotated['title_annotated'].str.contains(\"Q9332\")]\n",
    "print(behaviour_in_title[['id', 'title']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me the articles talking about \"social distancing\" : https://www.wikidata.org/wiki/Q30314010 in their headline:\n",
    "social_distancing_in_title = jsons_annotated[jsons_annotated['title_annotated'].str.contains(\"Q30314010\")]\n",
    "print(social_distancing_in_title[['id', 'title']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me the articles talking about \"Wuhan\" : https://www.wikidata.org/wiki/Q11746 in their headline:\n",
    "wuhan_in_title = jsons_annotated[jsons_annotated['title_annotated'].str.contains(\"Q11746\")]\n",
    "print(wuhan_in_title[['id', 'title']].to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
