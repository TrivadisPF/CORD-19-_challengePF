{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.7/site-packages (1.1.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.43.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: rdflib in /opt/conda/lib/python3.7/site-packages (4.2.2)\n",
      "Requirement already satisfied: SPARQLWrapper in /opt/conda/lib/python3.7/site-packages (1.8.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: isodate in /opt/conda/lib/python3.7/site-packages (from rdflib) (0.6.0)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from rdflib) (2.4.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode tqdm pandas rdflib SPARQLWrapper Unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path_header = r'CORD-19-research-challenge/2020-03-13/'):\n",
    "    import pandas as pd\n",
    "    path_to_csv = path_header + r\"all_sources_metadata_2020-03-13.csv\"\n",
    "    csv_df = pd.read_csv(path_to_csv)\n",
    "    return csv_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsons(path_header = r'CORD-19-research-challenge/2020-03-13/'):\n",
    "    import os, json\n",
    "    import pandas as pd\n",
    "\n",
    "    # this finds our json files\n",
    "    path_to_json = path_header + r'biorxiv_medrxiv/biorxiv_medrxiv'\n",
    "    path_to_json_2 = path_header + r\"comm_use_subset/comm_use_subset\"\n",
    "    path_to_json_3 = path_header + r\"noncomm_use_subset/noncomm_use_subset\"\n",
    "    path_to_json_4 = path_header + r\"pmc_custom_license/pmc_custom_license\"\n",
    "\n",
    "\n",
    "    list_of_jsons= [path_to_json, path_to_json_2, path_to_json_3,path_to_json_4]\n",
    "\n",
    "    json_files = []\n",
    "\n",
    "    for i in list_of_jsons:\n",
    "        json_files.extend([os.path.join(i,pos_json) for pos_json in os.listdir(i) if pos_json.endswith('.json')])\n",
    "\n",
    "\n",
    "\n",
    "    #json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "\n",
    "    # here I define my pandas Dataframe with the columns I want to get from the json\n",
    "    jsons_data = pd.DataFrame(columns=['id', \"title\", \"paper_abstract\",\"paper_body\"])\n",
    "\n",
    "    # we need both the json and an index number so use enumerate()\n",
    "\n",
    "    for index, js in enumerate(json_files):\n",
    "        with open(js) as json_file:\n",
    "            json_text = json.load(json_file)\n",
    "            #print(json_text)\n",
    "            # here you need to know the layout of your json and each json has to have\n",
    "            # the same structure (obviously not the structure I have here)\n",
    "            title = json_text['metadata']['title']\n",
    "\n",
    "            paper_id = json_text[\"paper_id\"]\n",
    "\n",
    "            #reduces the list only if there is content\n",
    "            if json_text[\"abstract\"] != []:\n",
    "                paper_abstract = (json_text[\"abstract\"])\n",
    "            else:\n",
    "                paper_abstract = (json_text[\"abstract\"])\n",
    "\n",
    "            #paper_abstract = (json_text[\"abstract\"])\n",
    "            paper_body = (json_text[\"body_text\"])\n",
    "            #print(title)\n",
    "            # here I push a list of data into a pandas DataFrame at row given by 'index'\n",
    "            jsons_data.loc[index] = [paper_id, title, paper_abstract, paper_body]\n",
    "\n",
    "    return jsons_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = read_jsons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the frames together\n",
    "combined_data = csv_df.merge(jsons, left_on='sha',right_on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv(\"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used this to buffer the extraction\n",
    "import pandas as pd\n",
    "combined_data = pd.read_csv(\"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a fulltest column\n",
    "# for this use the title_x (if Not NaN), title_x, all fields from the abstract, and all fields from the paper body together\n",
    "\n",
    "def parse_json_section(text):\n",
    "    import json\n",
    "    if isinstance(text, float) :\n",
    "        return \"\"\n",
    "    else :\n",
    "        sep = r\" \"\n",
    "        whole_text_as_string = \"\"\n",
    "        for section in text:\n",
    "            #print(section['text'])\n",
    "            whole_text_as_string += section['text'] + sep\n",
    "        return whole_text_as_string\n",
    "    \n",
    "\n",
    "def make_catchall_field(row):\n",
    "    sep = r\" \"\n",
    "    title_x = row['title_x']\n",
    "    title_y = row['title_y']\n",
    "    abstract = row['abstract']\n",
    "    abstract_fields = row['paper_abstract']\n",
    "    body_fields = row['paper_body']\n",
    "    abstract_text = parse_json_section(abstract_fields)\n",
    "    body_text = parse_json_section(body_fields)\n",
    "    \n",
    "    catch_all = str(title_x) + sep + str(title_y) + sep + str(abstract) + sep + str(abstract_text) + sep + str(body_text)\n",
    "    return catch_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "combined_data['catchall'] = combined_data.progress_apply(lambda row: make_catchall_field(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv(\"combined_data_with_catchall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used this to buffer the extraction\n",
    "import pandas as pd\n",
    "combined_data = pd.read_csv(\"combined_data_with_catchall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we need to annotate that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "# compile a regex that matches only alphanumeric plus the _\n",
    "import re\n",
    "pattern = re.compile('\\W')\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return str(re.sub(pattern, ' ', text)).encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "def analyze_text(text):\n",
    "    import requests \n",
    "    # do some cleansing of this text    \n",
    "    # defining the api-endpoint \n",
    "    API_ENDPOINT = 'http://nlu:8080/factextraction/analyze'\n",
    "    headers = {\"accept\": \"application/json\", \"content-type\": \"application/json\"}\n",
    "    # data to be sent to api\n",
    "    #print(\"Text sent to ambiverse is: \", remove_non_ascii(text))\n",
    "    data = r'{\"docId\": \"doc2\", \"text\":\"' + remove_non_ascii(text) + r'\", \"extractConcepts\": \"true\", \"language\": \"en\" }'\n",
    "    # sending post request and saving response as response object \n",
    "    r = requests.post(url = API_ENDPOINT, data = data, headers = headers) \n",
    "    #print(\"returned text is: \", r.text)\n",
    "    return r.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging only \n",
    "#from tqdm import tqdm\n",
    "#import swifter\n",
    "#jsons['out'] = jsons['title'].swifter.progress_bar(True).set_npartitions(npartitions=4).apply(analyze_text, axis=1)\n",
    "#jsons['title_annotated'] = jsons.swifter.allow_dask_on_strings().progress_bar(True).set_npartitions(npartitions=8).apply(lambda row: analyze_text(row['title']), axis = 1)\n",
    "\n",
    "#tqdm.pandas()\n",
    "#jsons['title_annotated'] = jsons[0:12].apply(lambda row: analyze_text(row['title']), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from here: https://www.kaggle.com/mlwhiz/parallelization-kernel\n",
    "from multiprocessing import  Pool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parallelized_apply(df, func, numProcs=4):\n",
    "    df_split = np.array_split(df, numProcs)\n",
    "    pool = Pool(numProcs) \n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def runAnnotate(df):\n",
    "    # add here also the functions for all other columns\n",
    "    df['catchall_annotated'] =  df.progress_apply(lambda row: analyze_text(row['catchall']), axis = 1)\n",
    "    return df\n",
    "\n",
    "# In theory every document annotation runs in a seperate thread, they all go to a single db (this db could handle a lot of parallel annotation threads)\n",
    "# Every annotation thread should have 8 GB of memory, so running 10 annotation threads in parallel would need 80 GB of memory. \n",
    "# Be careful with the memory monitor (e.g. htop), the db just shows the in memory indices as cached pages (yellor/orange bars in htop), \n",
    "# if you consume more memory elsewhere, then these pages are stored back to disk and you loose performance.\n",
    "\n",
    "# data_annotated = parallelized_apply(combined_data[0:399], runAnnotate, numProcs = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it first for the abstracts only\n",
    "abstracts_only = combined_data[combined_data.has_full_text != True]\n",
    "abstracts_annotated = parallelized_apply(abstracts_only, runAnnotate, numProcs = 12) \n",
    "abstracts_annotated.to_csv('abstracts_annotated.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/472 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# then for the fulltexts on a E32s v3 with 32 vCPUs and 256 GB of memory, we need around 50GB for the DB,\n",
    "# then we have around 100GB for NLU which should give us enough parallelism for 28 threads with large documents \n",
    "# and enough spare for python and the OS\n",
    "\n",
    "# do the first 100 for a test\n",
    "full_texts_only = combined_data[combined_data.has_full_text == True]\n",
    "full_texts_annotated = parallelized_apply(full_texts_only, runAnnotate, numProcs = 28)\n",
    "full_texts_annotated.to_csv('full_texts_annotated.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts_annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used this to buffer the extraction\n",
    "import pandas as pd\n",
    "abstracts_annotated = pd.read_csv(\"abstracts_annotated.csv\")\n",
    "full_texts_annotated = pd.read_csv(\"full_texts_annotated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_annotated = pd.concat(abstracts_annotated, full_texts_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to disk temporarly\n",
    "data_annotated.to_csv('data_annotated.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back the data\n",
    "import pandas as pd\n",
    "data_annotated = pd.read_csv(\"data_annotated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will want to get all further wikidata entity information to it, so we will need to load them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 16281/16281 [09:34<00:00, 28.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed:  14733263  triples into the graph\n",
      "Parsing failed on  69  items\n",
      "Found  1467541  wikidata links\n"
     ]
    }
   ],
   "source": [
    "# deassable the jsons first\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph\n",
    "from rdflib.namespace import RDF, FOAF, RDFS\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph_root = \"http://www.trivadis.com/kg/\"\n",
    "DCTYPE = Namespace(\"http://purl.org/dc/dcmitype/\")\n",
    "DCTERMS = Namespace(\"http://purl.org/dc/terms/\")\n",
    "\n",
    "# links from a document\n",
    "hasMention = URIRef(\"http://www.trivadis.com/kg/ontology/hasMention\")\n",
    "hasFact = URIRef(\"http://www.trivadis.com/kg/ontology/hasFact\")\n",
    "\n",
    "# links from a mention to the outside\n",
    "hasConcept = URIRef(\"http://www.trivadis.com/kg/ontology/hasConcept\")\n",
    "\n",
    "# types\n",
    "mentionType = URIRef(\"http://www.trivadis.com/kg/ontology/Mention\")\n",
    "docMentionType = URIRef(\"http://www.trivadis.com/kg/ontology/DocMention\")\n",
    "factType = URIRef(\"http://www.trivadis.com/kg/ontology/Fact\")\n",
    "conceptType = URIRef(\"http://www.trivadis.com/kg/ontology/Concept\")\n",
    "\n",
    "#mention properties\n",
    "hasCharLength = URIRef(\"http://www.trivadis.com/kg/ontology/hasCharLength\")\n",
    "hasCharOffset = URIRef(\"http://www.trivadis.com/kg/ontology/hasCharOffset\")\n",
    "hasText = URIRef(\"http://www.trivadis.com/kg/ontology/hasText\")\n",
    "hasConfidence = URIRef(\"http://www.trivadis.com/kg/ontology/hasConfidence\")\n",
    "hasName = URIRef(\"http://www.trivadis.com/kg/ontology/hasName\")\n",
    "hasURL = URIRef(\"http://www.trivadis.com/kg/ontology/hasURL\")\n",
    "hasType = URIRef(\"http://www.trivadis.com/kg/ontology/hasType\")\n",
    "\n",
    "# fact specifics\n",
    "hasSubject = URIRef(\"http://www.trivadis.com/kg/ontology/hasSubject\")\n",
    "hasRelation = URIRef(\"http://www.trivadis.com/kg/ontology/hasRelation\")\n",
    "hasObject = URIRef(\"http://www.trivadis.com/kg/ontology/hasObject\")\n",
    "\n",
    "\n",
    "# keys for the json\n",
    "match_key = 'matches'\n",
    "entity_key = 'entities'\n",
    "fact_key = 'facts'\n",
    "\n",
    "\n",
    "# some lists for further analysis or debuggung\n",
    "wikidata_uris = []\n",
    "error_parse = []\n",
    "\n",
    "def parse_row_to_graph(graph, row):\n",
    "    annotation = row[['catchall_annotated']][0]\n",
    "    doc_id = str(row[['sha']][0])\n",
    "    doi = row[['doi']][0]\n",
    "    catchall = row[['catchall']][0]\n",
    "\n",
    "    # create a unique document id\n",
    "    unique_id = str(hash(catchall + str(doc_id) + str(doi)))\n",
    "\n",
    "    #print(doc_id)\n",
    "    #print(catchall)\n",
    "    #print(doi)\n",
    "        \n",
    "    theDoc = URIRef(graph_root + urllib.parse.quote(unique_id))\n",
    "    g.add( (theDoc, RDF.type, DCTYPE.Text) )\n",
    "    g.add( (theDoc, DCTERMS.identifier, Literal(doi)) )\n",
    "    g.add( (theDoc, DCTERMS.description, Literal(catchall)) )\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        json_annotation = json.loads(annotation)\n",
    "\n",
    "        #print(json_annotation)\n",
    "\n",
    "        if match_key in json_annotation:\n",
    "            # add the annotations as mentions\n",
    "            # print(\"Found matches: \")\n",
    "            for match in json_annotation[match_key]:\n",
    "                # A match is: {'charLength': 7, 'charOffset': 0, 'text': 'Imaging', 'entity': {'id': 'http://www.wikidata.org/entity/Q931309', 'confidence': 0.12630685029203295}}\n",
    "                charLength = match['charLength']\n",
    "                charOffset = match['charOffset']\n",
    "                match_text = match['text']\n",
    "\n",
    "                match_iri = URIRef(graph_root + urllib.parse.quote(unique_id + \"/mention/\" + str(charLength) + str(charOffset) + str(match_text)))\n",
    "\n",
    "                g.add((theDoc, hasMention, match_iri))\n",
    "                g.add((match_iri, RDF.type, mentionType))\n",
    "                g.add((match_iri, hasCharLength, Literal(int(charLength))))\n",
    "                g.add((match_iri, hasCharOffset, Literal(int(charOffset))))\n",
    "                g.add((match_iri, hasText, Literal(match_text)))\n",
    "                if ('entity' in match):\n",
    "                    if ('id' in match['entity'] and 'confidence' in match['entity']):\n",
    "                        match_entity = match['entity']['id']\n",
    "                        wikidata_uris.append(match_entity)\n",
    "                        match_confidence = match['entity']['confidence']\n",
    "                        g.add((match_iri, hasConcept, URIRef(match_entity)))\n",
    "                        g.add((URIRef(match_entity), RDF.type, conceptType))\n",
    "                        g.add((match_iri, hasConfidence, Literal(float(match_confidence))))\n",
    "                # print(match)\n",
    "\n",
    "        if entity_key in json_annotation:\n",
    "            # add the annotations as mentions\n",
    "            # print(\"Found entities: \")\n",
    "            for entity in json_annotation[entity_key]:\n",
    "                # A entity is: {'id': 'http://www.wikidata.org/entity/Q931309', 'name': 'Medical imaging', 'url': 'http://en.wikipedia.org/wiki/Medical%20imaging', 'type': 'CONCEPT', 'salience': 0.0}\n",
    "                match_entity = entity['id']\n",
    "                wikidata_uris.append(match_entity)\n",
    "                match_name = entity['name']\n",
    "                match_url = entity['url']\n",
    "                match_type = entity['type']\n",
    "                match_confidence = entity['salience']\n",
    "                match_iri = URIRef(graph_root + urllib.parse.quote(unique_id + \"/entity/\" + str(match_name) + str(match_type)))\n",
    "\n",
    "                g.add((theDoc, hasMention, match_iri))\n",
    "                g.add((match_iri, RDF.type, docMentionType))\n",
    "                g.add((match_iri, hasConfidence, Literal(float(match_confidence))))\n",
    "                g.add((match_iri, hasName, Literal(match_name)))\n",
    "                g.add((match_iri, hasConcept, URIRef(match_entity)))\n",
    "                g.add((URIRef(match_entity), RDF.type, conceptType ))\n",
    "                g.add((match_iri, hasURL, Literal(match_url)))\n",
    "                g.add((match_iri, hasType, Literal(match_type)))\n",
    "                # print(entity)\n",
    "\n",
    "        if fact_key in json_annotation:\n",
    "            # add the annotations as facts\n",
    "            # print(\"Found facts: \")\n",
    "            for fact in json_annotation[fact_key]:\n",
    "                # A fact is: {'subject': {'text': 'RETRACTED Chinese medical staff', 'charOffset': 0, 'charLength': 32}, 'relation': {'text': 'request', 'charOffset': 33, 'charLength': 7}, 'object': {'text': 'international medical assistance in fighting against COVID-19 nan nan', 'charOffset': 41, 'charLength': 69}}\n",
    "                # in the fact we do not have any entities annotated, we need to do that later with the char offsets the fact has a subject, relation and object each of them are mentions\n",
    "\n",
    "                fact_subject_text = fact['subject']['text']\n",
    "                fact_subject_charLength = fact['subject']['charLength']\n",
    "                fact_subject_charOffset = fact['subject']['charOffset']\n",
    "\n",
    "                fact_relation_text = fact['relation']['text']\n",
    "                fact_relation_charLength = fact['relation']['charLength']\n",
    "                fact_relation_charOffset = fact['relation']['charOffset']\n",
    "\n",
    "                fact_iri = URIRef(graph_root + urllib.parse.quote(unique_id + \"/fact/\" + fact_relation_text +  str(fact_relation_charLength) +  str(fact_relation_charOffset)))\n",
    "                fact_subject_iri = URIRef(graph_root + urllib.parse.quote(unique_id + \"/fact/mention/\" + fact_subject_text +  str(fact_subject_charLength) +  str(fact_subject_charOffset)))\n",
    "                fact_relation_iri = URIRef(graph_root + urllib.parse.quote(unique_id + \"/fact/mention/\" + fact_relation_text +  str(fact_relation_charLength) +  str(fact_relation_charOffset)))\n",
    "\n",
    "\n",
    "                g.add((theDoc, hasFact, fact_iri))\n",
    "                g.add((fact_iri, RDF.type, factType))\n",
    "                g.add((fact_iri, hasSubject, fact_subject_iri))\n",
    "                g.add((fact_iri, hasRelation, fact_relation_iri))\n",
    "\n",
    "\n",
    "                g.add((fact_subject_iri, RDF.type, mentionType))\n",
    "                g.add((fact_subject_iri, RDF.type, factType))\n",
    "                g.add((fact_subject_iri, hasCharLength, Literal(int(fact_subject_charLength))))\n",
    "                g.add((fact_subject_iri, hasCharOffset, Literal(int(fact_subject_charOffset))))\n",
    "                g.add((fact_subject_iri, hasText, Literal(fact_subject_text)))\n",
    "\n",
    "                g.add((fact_relation_iri, RDF.type, mentionType))\n",
    "                g.add((fact_relation_iri, RDF.type, factType))\n",
    "                g.add((fact_relation_iri, hasCharLength, Literal(int(fact_relation_charLength))))\n",
    "                g.add((fact_relation_iri, hasCharOffset, Literal(int(fact_relation_charOffset))))\n",
    "                g.add((fact_relation_iri, hasText, Literal(fact_relation_text)))\n",
    "\n",
    "                # adding the object if it exists\n",
    "                if ('object' in fact):\n",
    "                    if ('text' in fact['object'] and 'charLength' in fact['object'] and 'charOffset' in fact['object']):\n",
    "                        fact_object_text = fact['object']['text']\n",
    "                        fact_object_charLength = fact['object']['charLength']\n",
    "                        fact_object_charOffset = fact['object']['charOffset']\n",
    "                        fact_object_iri = URIRef(graph_root + urllib.parse.quote(unique_id + \"/fact/mention/\" + fact_object_text +  str(fact_object_charLength) +  str(fact_object_charOffset)))\n",
    "                        g.add((fact_iri, hasObject, fact_object_iri))\n",
    "                        g.add((fact_object_iri, RDF.type, mentionType))\n",
    "                        g.add((fact_object_iri, RDF.type, factType))\n",
    "                        g.add((fact_object_iri, hasCharLength, Literal(int(fact_object_charLength))))\n",
    "                        g.add((fact_object_iri, hasCharOffset, Literal(int(fact_object_charOffset))))\n",
    "                        g.add((fact_object_iri, hasText, Literal(fact_object_text)))\n",
    "\n",
    "                # print(fact)\n",
    "\n",
    "    except:\n",
    "        if doc_id != \"nan\":\n",
    "            error_parse.append(doc_id)\n",
    "\n",
    "            \n",
    "g = Graph()\n",
    "# just using two documents\n",
    "# abstracts_annotated[0:2].progress_apply(lambda row: parse_row_to_graph(g, row), axis = 1)\n",
    "\n",
    "# running on all:\n",
    "data_annotated.progress_apply(lambda row: parse_row_to_graph(g, row), axis = 1)\n",
    "\n",
    "print(\"Parsed: \", len(g), \" triples into the graph\")\n",
    "print(\"Parsing failed on \", len(error_parse), \" items\")\n",
    "print(\"Found \", len(wikidata_uris), \" wikidata links\")\n",
    "g.serialize(destination='full_annotated.nt', format='nt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 Items by occurence are:  [('http://www.wikidata.org/entity/Q808', 26906), ('http://www.wikidata.org/entity/Q166231', 20885), ('http://www.wikidata.org/entity/Q7868', 14800), ('http://www.wikidata.org/entity/Q8054', 14509), ('http://www.wikidata.org/entity/Q12136', 10279), ('http://www.wikidata.org/entity/Q101965', 9686), ('http://www.wikidata.org/entity/Q103177', 7514), ('http://www.wikidata.org/entity/Q221673', 7311), ('http://www.wikidata.org/entity/Q7187', 6920), ('http://www.wikidata.org/entity/Q42848', 6556)]\n"
     ]
    }
   ],
   "source": [
    "# deassamble the wikidata links to be able to load them from wikidata\n",
    "from collections import Counter\n",
    "wikidata_uris_testing = ['http://www.wikidata.org/entity/Q931309', 'http://www.wikidata.org/entity/Q11426']\n",
    "\n",
    "occurence_by_item = Counter(wikidata_uris)\n",
    "# print out the most common 10\n",
    "print(\"The top 10 Items by occurence are: \", occurence_by_item.most_common(10))\n",
    "\n",
    "# we are interested in just the distinct list of them\n",
    "distinct_wd_items = dict(occurence_by_item).keys()\n",
    "\n",
    "# split it to fixed size chunks so we do not overload wikidata\n",
    "# copied from: https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.wikidata.org/entity/Q931309',\n",
       " 'http://www.wikidata.org/entity/Q12192',\n",
       " 'http://www.wikidata.org/entity/Q167918',\n",
       " 'http://www.wikidata.org/entity/Q7316896',\n",
       " 'http://www.wikidata.org/entity/Q6813432',\n",
       " 'http://www.wikidata.org/entity/Q14970638',\n",
       " 'http://www.wikidata.org/entity/Q41117',\n",
       " 'http://www.wikidata.org/entity/Q884',\n",
       " 'http://www.wikidata.org/entity/Q11746',\n",
       " 'http://www.wikidata.org/entity/Q148']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(distinct_wd_items)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from string import Template\n",
    "\n",
    "sparql_endpoint_url = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql\"\n",
    "sparql = SPARQLWrapper(sparql_endpoint_url)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "\n",
    "def QuerySparql(sparql_query):\n",
    "    sparql.setQuery(sparql_query)\n",
    "    results = sparql.query().convert()\n",
    "    return json_normalize(results[\"results\"][\"bindings\"])\n",
    "\n",
    "\n",
    "\n",
    "#usage of Sparql-Query as a template:\n",
    "wd_get_instance_of = Template(\"\"\"\n",
    "PREFIX bis: <http://www.bis.org/ontology/>\n",
    "PREFIX gleifL1: <https://www.gleif.org/ontology/L1/>\n",
    "PREFIX gleifL2: <https://www.gleif.org/ontology/L2/>\n",
    "PREFIX gleif: <https://www.gleif.org/ontology/Base/>\n",
    "PREFIX emaxx: <https://emaxxreuters.com/>\n",
    "\n",
    "    \n",
    "} \n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "wd_get_subclass_of\n",
    "\n",
    "\n",
    "wd_get_label_of\n",
    "\n",
    "bis_entities =  QuerySparql(query1.substitute(COUNTRYNAME=str(country), AMOUNT=str(amount), LIMIT=str(limit)))\n",
    "\n",
    "list(distinct_wd_items)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add some analytics after this cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the instance of (P31) and the subclass of (P279) relations along each path from wikidata, we need all intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And ask the questions on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(jsons_annotated.iloc[13000:13001].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me the articles talking about \"behaviour\" : https://www.wikidata.org/wiki/Q9332 in their headline:\n",
    "behaviour_in_title = jsons_annotated[jsons_annotated['title_annotated'].str.contains(\"Q9332\")]\n",
    "print(behaviour_in_title[['id', 'title']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me the articles talking about \"social distancing\" : https://www.wikidata.org/wiki/Q30314010 in their headline:\n",
    "social_distancing_in_title = jsons_annotated[jsons_annotated['title_annotated'].str.contains(\"Q30314010\")]\n",
    "print(social_distancing_in_title[['id', 'title']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me the articles talking about \"Wuhan\" : https://www.wikidata.org/wiki/Q11746 in their headline:\n",
    "wuhan_in_title = jsons_annotated[jsons_annotated['title_annotated'].str.contains(\"Q11746\")]\n",
    "print(wuhan_in_title[['id', 'title']].to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
